## 为什么HashMap的put函数把hash冲突元素放至链表的表头？

​		因为放置末尾的话，还需要从head遍历到末尾，直接放至表头避免的尾部遍历，提升了一定性能

## HashMap什么情况下会出现死循环，出现死循环的大致过程是怎样的？

​		由于HashMap在扩容的时候会对每个元素进行重新hash，然后放置到扩容后的新Map中，而重新hash计算之后，节点之间的顺序会发生变化，特别是在多线程环境下，可能链表节点A->B->null，然后另一个线程通过重新hash计算之后，链表节点可能变为B->A，此时链表就形成了一个环，那么此时如果你通过get函数获取元素时，假如刚好获取的就是这个发生冲突的key，那么在遍历这个链表时就会陷入死循环。在JDK1.8中对此进行了优化，
通过增加tail指针，既避免了死循环问题（让数据直接插入到队尾），又避免了尾部遍历。

## 如何避免缓存穿透、缓存雪崩、缓存击穿？

**缓存穿透**
		缓存穿透是指查询一个一定不存在的数据，由于缓存是不命中时被动写的，并且出于容错考虑，如果从存储层查不到数据则不写入缓存，这将导致这个不存在的数据每次请求都要到存储层去查询，失去了缓存的意义。在流量大时，可能DB就挂掉了，要是有人利用不存在的key频繁攻击我们的应用，这就是漏洞。

**解决方案**
		有很多种方法可以有效地解决缓存穿透问题，最常见的则是采用布隆过滤器，将所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据会被这个bitmap拦截掉，从而避免了对底层存储系统的查询压力。另外也有一个更为简单粗暴的方法，如果一个查询返回的数据为空（不管是数据不存在，还是系统故障），我们仍然把这个空结果进行缓存，但它的过期时间会很短，最长不超过五分钟。

**缓存雪崩**
		缓存雪崩是指在我们设置缓存时采用了相同的过期时间，导致缓存在某一时刻同时失效，请求全部转发到DB，DB瞬时压力过重雪崩。

**解决方案**
		缓存失效时的雪崩效应对底层系统的冲击非常可怕。大多数系统设计者考虑用加锁或者队列的方式保证缓存的单线 程（进程）写，从而避免失效时大量的并发请求落到底层存储系统上。这里分享一个简单方案就是将缓存失效时间分散开，比如我们可以在原有的失效时间基础上增加一个随机值，比如1-5分钟随机，这样每一个缓存的过期时间的重复率就会降低，就很难引发集体失效的事件。

**缓存击穿**
		对于一些设置了过期时间的key，如果这些key可能会在某些时间点被超高并发地访问，是一种非常“热点”的数据，这个时候，需要考虑一个问题：缓存被“击穿”的问题，这个和缓存雪崩的区别在于这里针对某一key缓存，前者则是很多key。缓存在某个时间点过期的时候，恰好在这个时间点对这个Key有大量的并发请求过来，这些请求发现缓存过期一般都会从后端DB加载数据并回设到缓存，这个时候大并发的请求可能会瞬间把后端DB压垮。

**解决方案**
*使用互斥锁(mutex key)*
		业界比较常用的做法，是使用mutex。简单地来说，就是在缓存失效的时候（判断拿出来的值为空），不是立即去load db，而是先使用缓存工具的某些带成功操作返回值的操作（比如Redis的SETNX或者Memcache的ADD）去set一个mutex key，当操作返回成功时，再进行load db的操作并回设缓存；否则，就重试整个get缓存的方法。

```java
public String get(key) {
String value = redis.get(key);
if (value == null) { //代表缓存值过期
//设置3min的超时，防止del操作失败的时候，下次缓存过期一直不能load db
String keynx = key.concat(":nx");
if (redis.setnx(keynx, 1, 3 * 60) == 1) { //代表设置成功
value = db.get(key);
redis.set(key, value, expire_secs);
redis.del(keynx);
} else {
//这个时候代表同时候的其他线程已经load db并回设到缓存了，这时候重试获取缓存值即可
sleep(50);
get(key); //重试
}
} else {
return value; 
}
}
```

## String类为什么被设计为final?

​		在并发场景下，多个线程同时读一个资源，是不会引发竟态条件的。只有对资源做写操作才有危险。不可变对象不能被写，所以线程安全。
​		不可变性支持字符串常量池,这样在大量使用字符串的情况下，可以节省内存空间，提高效率。但之所以能实现这个特性，String的不可变性是最基本的一个必要条件。恰好字符串类型是我们日常编码使用最多的一种数据类型。

## 为什么要使用消息队列，消息队列的使用场景有哪些？

1. 系统之间解耦(比如A系统需要调用B,C,D系统的接口，突然某条需求变更，需要增加一个E系统，甚至某天又要求去掉对B系统接口的调用，那么A系统需要时刻维护对其他系统服务接口的调用处理，然而消息队列可以实现系统之间的解耦，从而使得A系统与其他系统之间无需关心彼此的存在，各自系统的需求变更不会影响到对方系统)
2. 异步化(比如需要往A/B/C3个数据库写入数据，同时数据写入不要求实时性，那么就可以通过消息队列实现数据的异步写入，加快主业务流程的响应时间)
3. 流量削峰(当客户端请求并发量太大，超过了机器最大可处理能力时，可以将请求放入消息队列进行排队，依次处理，这样系统就不会过载，不会造成系统阻塞。)

## 说一说消息队列的优缺点？

优点：解耦、异步化、削峰
缺点：

1. 系统可用性降低，需要额外维护一套MQ系统的可用性，保证MQ系统可用性99.99999%，防止MQ系统挂掉
2. 系统复杂性增加，引入了MQ之后，你需要考虑的问题变多了，比如你如何保证消费不会被重复消费，如何保证消息不会丢失，如何实现消息的顺序性？
3. 引入了消息一致性问题：在自动消息确认模式下，在消息还没有消费时，若消费者挂掉了，由于MQ在auto_acknowledge下，当前生产者不会重新发送该消息，这就产生了消息不一致的情况。

## 如何保证接口的幂等性？

1. 查询操作：查询一次和查询多次，在数据不变的情况下，查询结果是一样的。select是天然的幂等操作；

2. 删除操作：删除操作也是幂等的，删除一次和多次删除都是把数据删除。(注意可能返回结果不一样，删除的数据不存在，返回0，删除的数据多条，返回结果多个) ；

3. 唯一索引：防止新增脏数据。比如：支付宝的资金账户，支付宝也有用户账户，每个用户只能有一个资金账户，怎么防止给用户创建资金账户多个，那么给资金账户表中的用户ID加唯一索引，所以一个用户新增成功一个资金账户记录。要点：唯一索引或唯一组合索引来防止新增数据存在脏数据（当表存在唯一索引，并发时新增报错时，再查询一次就可以了，数据应该已经存在了，返回结果即可）；

4. token机制：防止页面重复提交。

   1. 原理上通过session token来实现的(也可以通过redis来实现)。当客户端请求页面时，服务器会生成一个随机数Token，并且将Token放置到session当中，然后将Token发给客户端（一般通过构造hidden表单）。
      下次客户端提交请求时，Token会随着表单一起提交到服务器端。
   2. 服务器端第一次验证相同过后，会将session中的Token值更新下，若用户重复提交，第二次的验证判断将失败，因为用户提交的表单中的Token没变，但服务器端session中Token已经改变了。

5. 悲观锁

   获取数据的时候加锁获取。select * from table_xxx where id='xxx' for update; 注意：id字段一定是主键或者唯一索引，不然是锁表，会死人的；悲观锁使用时一般伴随事务一起使用，数据锁定时间可能会很长，根据实际情况选用；

6. 乐观锁——乐观锁只是在更新数据那一刻锁表，其他时间不锁表，所以相对于悲观锁，效率更高。乐观锁的实现方式多种多样可以通过version或者其他状态条件：

   1. 通过版本号实现update table_xxx set name=#name#,version=version+1 where version=#version#如下图(来自网上)；
   2. 通过条件限制 update table_xxx set avai_amount=avai_amount-#subAmount# where avai_amount-#subAmount# >= 0要求：quality-#subQuality# >= ，这个情景适合不用版本号，只更新是做数据安全校验，适合库存模型，扣份额和回滚份额，性能更高；

7.  分布式锁

   如果是分布是系统，构建全局唯一索引比较困难，例如唯一性的字段没法确定，这时候可以引入分布式锁，通过第三方的系统(redis或zookeeper)，在业务系统插入数据或者更新数据，获取分布式锁，然后做操作，之后释放锁，这样其实是把多线程并发的锁的思路，引入多多个系统，也就是分布式系统中得解决思路。要点：某个长流程处理过程要求不能并发执行，可以在流程执行之前根据某个标志(用户ID+后缀等)获取分布式锁，其他流程执行时获取锁就会失败，也就是同一时间该流程只能有一个能执行成功，执行完成后，释放分布式锁(分布式锁要第三方系统提供)；

8. select + insert

   并发不高的后台系统，或者一些任务JOB，为了支持幂等，支持重复执行，简单的处理方法是，先查询下一些关键数据，判断是否已经执行过，在进行业务处理，就可以了。注意：核心高并发流程不要用这种方法；

9.  状态机幂等

   在设计单据相关的业务，或者是任务相关的业务，肯定会涉及到状态机(状态变更图)，就是业务单据上面有个状态，状态在不同的情况下会发生变更，一般情况下存在有限状态机，这时候，如果状态机已经处于下一个状态，这时候来了一个上一个状态的变更，理论上是不能够变更的，这样的话，保证了有限状态机的幂等。注意：订单等单据类业务，存在很长的状态流转，一定要深刻理解状态机，对业务系统设计能力提高有很大帮助

10.  对外提供接口的api如何保证幂等

    1. 如银联提供的付款接口：需要接入商户提交付款请求时附带：source来源，seq序列号；source+seq在数据库里面做唯一索引，防止多次付款(并发时，只能处理一个请求) 。
    2. 重点：对外提供接口为了支持幂等调用，接口有两个字段必须传，一个是来源source，一个是来源方序列号seq，这个两个字段在提供方系统里面做联合唯一索引，这样当第三方调用时，先在本方系统里面查询一下，是否已经处理过，返回相应处理结果；没有处理过，进行相应处理，返回结果。注意，为了幂等友好，一定要先查询一下，是否处理过该笔业务，不查询直接插入业务系统，会报错，但实际已经处理了。

## Kafka如何保证高可用的？

​		Broker相当于一个节点，一般一个节点处于一台机器上，一个topic会划分为多个partition，每个partition分散在多个broker节点上，然后每个partition还会有多个副本，来保证partition数据不会由于某个broker故障而丢失，从而确保了Kafka高可用。

## 如何保证消息队列里的消息不会被重复消费？

​		目前任何消息队列都不保证消息不会重复发送，需要客户端自己解决消息重复消费问题。以Kafka为例，消费者消费消息后，会定时定期提交offset至ZK(不是每消费一条消息就立即提交offset)，而kafka会监听ZK上的offset值。但是当消费者准备提交offset，然而还未提交offset成功时，若消费者此时恰好重启了，这样消费者的offset值就会丢失，这样kafka就不会感知到消费者当前最新的offset值，这样就会导致kafka会从旧的offset开始发送消息，从而导致了重复发送消息，然后消费者就会重复消费该消息。某些业务可能会由于消息的重复消费而发生数据异常，比如通知扣款消息消费两次，就会导致对用户账户资产进行重复扣款。
由于消息队列天然就不保证消息不会重复发送，因为客户端需要自己保证接口的幂等性，即接口调用N次，最终产生的结果和状态是一样的。比如可以客户端维护一个全局消息表状态表，记录每个消息的消费状态，这要求你记录每次业务的唯一ID、消息ID、消息的消费状态。

## 如何保证消息队列不会丢消息？

1. 消息丢失分两种：生产者弄丢的和消费者弄丢的、MQ自己弄丢的。

   ​		生产者将消息发送到MQ时，因网络传输问题未发送到MQ而导致丢失

   解决方案：

   1. 通过对消息发送代码进行try-catch，在catch中进行rollback，但前提是该MQ需要支持事务机制，此外事务机制是同步操作，会降低消息发送的吞吐量，一般不建议使用这种方式。
   2. 开启channel的confirm模式，它是异步发送消息，MQ接收到消息后会以异步回调方式通知生产者：消息我接收成功or失败了。

   Kafka生产者丢消息的两种情况：
   1. 使用同步模式的时候，有3种状态保证消息被安全生产，在配置为1（只保证写入leader成功）的话，如果刚好leader partition挂了，数据就会丢失。
   2. 还有一种情况可能会丢失消息，就是使用异步模式的时候，当缓冲区满了，如果配置为0（还没有收到确认的情况下，缓冲池一满，就清空缓冲池里的消息），数据就会被立即丢弃掉。
   对于Kafka的解决方案：
      1. 就是说在同步模式的时候，确认机制设置为-1，也就是让消息写入leader和所有的副本。
      2. 在异步模式下，如果消息发出去了，但还没有收到确认的时候，缓冲池满了，在配置文件中设置成不限制阻塞超时的时间，也就说让生产端一直阻塞，这样也能保证数据不会丢失。

2. 消费者弄丢的

   ​		消费者消费完消息之后一般还需要根据消费到的消息进行一些业务处理，若这些业务处理中途失败了，但若你开启自动ack机制，那么MQ会认为消费者已经消费完消息了，然后MQ会将该消息删除，从而导致消息丢失。
   解决方案：
   ​		在消费者端，你应该关闭自动ack机制，开启手动ack，即你自己处理完自己的业务之后，再手动ack，从而保证消费者真正把消息消费了，正确ack之后，MQ也就会正确的删除消息，从而保证消息不会丢失。
   对于Kafka而言，也是类似的，你只需要使用手动提交offset，而非自动提交offset即可。

3. MQ自己弄丢的

   ​		MQ接收到生产者发送过来的消息时，它首先会将消息暂存在本地内存中，假如MQ还未来得及将本地内存中暂存的消息持久化的话，那么就会导致消息丢失。
   ​		对于Kafka而言，当生产者发送一条消息到Kafka，可能该消息存储在一个partition的leader上，但该leader尚未将该消息数据同步到其他副本上，恰好此时该Leader宕机了，这样就会导致消息丢失。
   解决方案：
   ​		因此我们必须通过配置要求MQ将消息持久化到磁盘，比如rabbitMQ需要将queue设置为需要持久化，这样就会保证queue的元数据会被持久化，然后需要再发送消息端设置deliverMode为持久化，这样会保证queue里的消息会被持久化，从而保证了MQ自身不会丢失消息。
   针对Kafka的解决方案需要设置几个参数，具体如下：
   block.on.buffer.full = true
   acks = all
   retries = MAX_VALUE
   max.in.flight.requests.per.connection = 1
   使用KafkaProducer.send(record, callback)
   callback逻辑中显式关闭producer：close(0) 
   unclean.leader.election.enable=false
   replication.factor = 3 
   min.insync.replicas = 2
   replication.factor > min.insync.replicas
   enable.auto.commit=false
   切记：消息处理完成之后再手动提交offset！

## 如何保证消息队列中消息的顺序？

会发生消息顺序错乱的两个场景

1. RabbitMQ：
   一个生产者，多个消费者情况下，由于消费者的消费顺序不确定性，从而导致了消息的消费顺序跟消费的生产顺序不一致。
   解决方案：
   将消息发送到MQ的不同queue队列中，让不同消费者消费不同的queue，然后将需要保证消费顺序的消息发送到同一个queue队列中，从而保证这些消息会按照发送顺序依次被同一个消费者所消费。
2. Kafka：
   一个topic，一个partition，一个消费者(内部多线程)
   首先Kafka已经保证了写入到一个partition中的数据一定是有顺序的，而且一个partition只会被同一个消费组中的一个消费者消费。因此生产者只需要为消息指定一个业务key(比如订单id)，这样就保证了同一个key的消息一定会写入到同一个partition中，由于partition中数据天然就是有序的，只要保证单个消费者中是单线程方式消费消息的，那么消息一定会被按照生产顺序依次被消费。但实际使用场景中，我们往往为了提升消费者消费消息的性能，会使用多线程方式来并发消费消息，从而会导致消息的消费顺序错乱。那么Kafka中该如何保证消息被顺序消费呢？
   解决方案：
   比如你消息的key设置为业务线key，那么你需要为每个业务线创建一个基于内存的队列，保证同一个parition的消息会被分发到同一个内存队列中，这样内存队列中的消息数据同它对应的partition中的消息顺序一致，然后某一个线程处理一个内存队列中的消息数据，避免多个线程处理同一个内存队列，这样就能保证消息会被顺序消费。

## 简述JMM

JMM即Java Memory Model，翻译过来就是Java内存模型，是一种抽象概念，并非真实存在。它描述的是一组规则或规范，通过这组规范定义了程序中各个变量(比如类的成员变量、静态属性等等)的访问方式。
JMM关于同步的规定：
1. 线程解锁前，必须将共享变量的值刷新回主内存
2. 线程加锁前，必须读取主内存中共享变量的最新值到自己的工作内存中
3. 线程加锁解锁必须是同一把锁

## JMM的3大特性：

1. 可见性
   一个线程将自己工作内存内共享变量的最新值刷新回主内存，其他线程能收到主内存中共享变量值发生改变的通知，并能看到最新的改变，这种性质即JMM中的可见性。Java里的可见性是通过底层的Lock前缀的指令以及缓存一致性协议(比如MESI)实现的。
2. 原子性
   一个线程在进行某个操作过程中，不可被打断，不可被分割，它要么执行成功，要么执行失败，不可处于一种中间状态。要求原子性是为了保证数据的完整性。Java里的原子性是通过CAS实现的。
3. 有序性
   计算机执行程序时，为了提高执行性能，编译期和处理器常常会对指令进行重排序，一般分以下3种：
   源代码-->编译期优化的重排-->指令并行的重排-->内存系统的重排-->最终执行的指令
   Java里的有序性是通过内存屏障实现的。
   内存屏障有两个作用：
   A.阻止屏障两侧的指令发生重排序；
   B.强制把写缓冲区/高速缓存中的脏数据等写回主内存，让缓存中相应的数据失效。
   硬件层的内存屏障分为两种：Load Barrier 和 Store Barrier即读屏障和写屏障。
   Java的内存屏障通常所谓的四种:
   LoadLoad,StoreStore,LoadStore,StoreLoad
   实际上也是上述两种的组合，完成一系列的屏障和数据同步功能。
   LoadLoad屏障：对于这样的语句Load1; LoadLoad; Load2，在Load2及后续读取操作要读取的数据被访问前，保证Load1要读取的数据被读取完毕。
   StoreStore屏障：对于这样的语句Store1; StoreStore; Store2，在Store2及后续写入操作执行前，保证Store1的写入操作对其它处理器可见。
   LoadStore屏障：对于这样的语句Load1; LoadStore; Store2，在Store2及后续写入操作被刷出前，保证Load1要读取的数据被读取完毕。
   StoreLoad屏障：对于这样的语句Store1; StoreLoad; Load2，在Load2及后续所有读取操作执行前，保证Store1的写入对所有处理器可见。它的开销是四种屏障中最大的。在大多数处理器的实现中，这个屏障是个万能屏障，兼具其它三种内存屏障的功能

Java中的volatile的内存屏障策略非常严格保守，非常悲观且毫无安全感的心态：
1.在每个volatile写操作前插入StoreStore屏障，在写操作后插入StoreLoad屏障；
2.在每个volatile读操作前插入LoadLoad屏障，在读操作后插入LoadStore屏障；

Java中final关键字的内存屏障
对于final域，编译器和CPU会遵循两个排序规则：
1.新建对象过程中，构造体中对final域的初始化写入和这个对象赋值给其他引用变量，这两个操作不能重排序；
2.初次读包含final域的对象引用和读取这个final域，这两个操作不能重排序；（晦涩，意思就是先赋值引用，再调用final值）

总之上面规则的意思可以这样理解，必需保证一个对象的所有final域被写入完毕后才能引用和读取。这也是内存屏障的起的作用：
写final域：在编译器写final域完毕，构造体结束之前，会插入一个StoreStore屏障，保证前面的对final写入对其他线程/CPU可见，并阻止重排序。
读final域：在上述规则2中，两步操作不能重排序的机理就是在读final域前插入了LoadLoad屏障。
X86处理器中，由于CPU不会对写-写操作进行重排序，所以StoreStore屏障会被省略；而X86也不会对逻辑上有先后依赖关系的操作进行重排序，所以LoadLoad也会被省略

## 谈谈happens-before规则

程序顺序规则：一个线程中的每个操作，happens-before于该线程中的任意后续操作。
监视器锁规则：对一个锁的解锁，happens-before于随后对这个锁的加锁。
volatile规则：对一个volatile域的写，happens-before于任意后续对这个volatile域的读。
传递性：如果Ahappens-before B，并且B happens-before C，那么A happens-before C。
start()规则：如果线程A执行操作ThreadB.start()，那么A线程的ThreadB.start()操作happens-before于线程B中的任意操作。
join()规则：如果线程A执行操作ThreadB.join()并成功返回，那么线程B的任意操作happens-before于线程A从ThreadB.join()操作成功返回。
注意：A happens-before B并不意味着A一定要先在B之前发生，而是说，如果A已经发生在了B前面，那么A的操作结果一定要对B可见

## 什么是CAS？

​		CAS全程Compare-And-Swap,它是一条CPU并发原语，CPU原语是由若干个CPU指令组成，用于完成某个功能，而CPU原语必须连续执行，在执行过程中不可中断，也就是说CAS相当于一条CPU原子指令，CPU天然就保证了它的原子性，从而保证了数据一致性。
Java是通过UnSafe类封装了对底层CPU原语的调用。