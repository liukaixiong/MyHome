# 概述

## 交易型系统设计的一些原则

### 高并发原则

#### 无状态

不同的机房需要读取不同的数据源，此时就需要通过配置文件或配置中心指定

#### 拆分

**系统纬度**:按照系统功能/业务拆分，比如商品系统、购物车、结算、订单系统等。

**功能纬度**:对一个系统进行功能再拆分，比如优惠券系统可以拆分为后台券创建系统,领券系统、用券系统

**读写纬度**: 根据读写比例进行拆分。比如商品系统，交易的各个系统都会读取数据，读的量大于写，因此可以拆分成商品写服务、商品读服务；

> 读服务可以考虑用缓存提升性能；写的量太大时，可以考虑分库分表；有些聚合读取的场景，如商品详情页，可以考虑数据异构拆分系统，将分散在多处的数据聚合到一处存储，以提升系统的性能和可靠性。

**AOP纬度**: 根据访问特征,按照AOP进行拆分，比如商品详情页可以分为CDN、页面渲染系统；CDN就是一个AOP系统。

**模块纬度**: 代码结构按照三层架构(web、service、dao)进行划分

#### 服务化

访问量太大，导致把整个服务打挂，因此需要为不同的调用方提供不同的服务分组，隔离访问

调用量的上升还要考虑服务的限流、黑白名单等。

超时时间、重试机制、服务路由(动态切换不同的分组)、故障补偿。

#### 消息队列

解耦/异步处理/流量削峰/缓冲。

如果订阅者太多,那么订阅单个消息队列就会成为瓶颈,此时需要考虑对消息队列进行多个镜像复制。

注意处理生产消息失败，以及消息的重复接收的场景。

消息重试，多次重试提示失败。

持久化消息,增加报警，防重复处理。

##### 大流量缓冲

流量过大时，牺牲强一致性，保持最终一致性。

库存案例:

 	1. redis扣减库存
		2. 记录扣减日志
		3. 同步work
		4. 库存db

> 直接在redis中扣减，然后记录扣减日志，通过worker同步到db

订单交易设计:

1. 结算服务

2. 接单服务

   1. 订单redis

      1.1 订单状态机

   2. 订单队列表

#### 数据异构

##### 数据异构

按照订单编号进行分库分表。

##### 数据闭环

1. 通过MQ机制接受数据变更，然后原子化到合适的存储引擎。如Redis或持久化KV存储
2. 将多个数据源的数据拿过来，然后通过聚合步骤存储到指定的数据引擎中

#### 缓存银弹

- 浏览器缓存

设置过期时间，如对相应头Expires、Cache-control进行控制。

**仅仅针对实时性不敏感的数据**

- App客户端缓存

提前将需要的素材缓存到app中。网络异常的情况下还是要有托底的数据给app使用。

- CDN缓存

页面、活动页、图片等服务可以考虑将页面、活动页、图片推送到离用户最近的CDN节点，让用户能在离他最近的节点找到最想要的数据。

- 接入层缓存

URL重写: 去除随机数

一致性哈希: 按照指定的参数做一致性hash，从而保证相同的数据落到一台服务器上。

- 应用层缓存

分为堆内缓存、堆外缓存。

堆内缓存重启自动消失。

堆外缓存可以考虑使用local redis cache。

local redis cache : 通过再应用所在服务器上部署一组redis，应用直接读取`本地`redis获取数据。

多机之间使用主从机制同步数据。这种方式没有网络消耗，性能是最优的。

- 分布式缓存

数据量不大的情况下可以考虑上面的local redis cache架构。

在数据量大的情况下，单机的缓存有限。可以使用分片机制将流量分散到多台，或者直接使用分布式缓存使用。

常见的分片规则有:一致性哈希

#### 并发化

没有依赖的数据，可以并行去获取。
例如 A - B -C -D - E 。 A - C - E / B / D ，这样的话可以将5个步骤拆分成三步。并发获取。

## 高可用原则

### 降级

#### 开关集中化管理

通过配置中心的推送机制把开关推送到各个应用。

#### 可降级的多级读服务

通过降级来通知应用是否读取本地缓存/分布式缓存/托底数据

#### 开关前置化

将开关设置到Nginx中，请求流量不流到后端tomcat中。

#### 业务降级

将同步改为异步，并保障数据的最终一致性。

高并发流量来袭时，确保优先级处理高的数据，合理分配进入系统的流量。

### 限流

限流的目的是防止恶意请求流量、恶意攻击，或者防止流量超出系统峰值。

1. 恶意请求流量只访问到db
2. 对于穿透到后端的流量可以考虑使用nginx的limit模块处理
3. 对于恶意ip进行屏蔽

原则是为了保护应用的正常使用。

### 切流量

当服务或者应用挂掉了的时候，能快速切换到可用的服务器。

1. DNS : 切换机房入口
2. HttpDNS :  APP场景，在客户端分配好流量入口，绕过运营商localDNS并实现更精准的调度。
3. LVS/HaProxy : 切换故障的Nginx接入层
4. Nginx : 切换故障的应用层.

### 可回滚

通过版本、数据进行回滚。



### 测试指标

- 吞吐量
- 响应时间
- 可用性
- 降级方案

### 监控指标

- 机器负载
- 相应时间
- 可用率

### 应急指标

- 容灾
- 降级
- 限流
- 隔离
- 切流量
- 可回滚

## 负载均衡与反向代理

### upstream配置

**负载均衡**

```tex
upstream backend{
    server ip:port weight=2
    server ip:port weight=5
}
```

权重(weight):  默认 1 ,权重越高,分配的几率越大

通过

```tex
location / {
    proxy_pass http://backend;
}
```

### 负载均衡算法

#### round-robin : 轮询。

ip_hash : 根据客户端IP进行Hash去摸,即同一个ip肯定会被负载到同一台上面

```tex
upstream backend{
	ip_hash;
    server ip:port weight=2
    server ip:port weight=5
}
```

 hash key : 对某一个key使用hash或者一致性hash算法进行负载均衡. 但是普通的hash算法会存在缺点.当某机器宕机了的情况下,导致所有的hash将要重新进行分配，这种情况下会建议使用一致性hash。即使出现宕机情况，也只会影响小部分机器。

#### url hash

```tex
upstream backend{
	hash $uri;
    server ip:port weight=2
    server ip:port weight=5
}
```

#### 一致性哈希

consistent_key动态指定。

```tex
upstream backend{
	hash $consistent_key consistent;
    server ip:port weight=2
    server ip:port weight=5
}
```

### 失败重试

两部分配置 : upstream server 和 proxy_pass

```tex
upstream backend{
    server ip:port max_fails=2 fail_timeout=10s weight=2
    server ip:port max_fails=2 fail_timeout=10s weight=5
}
```

当fail_timeout时间内失败了max_fails次请求，则认为该上游服务器不可用/不存活，然后摘掉该上游服务器，fail_timeout时间会再次将该服务加入到存活服务器列表进行重试。

```tex
location /test{
    proxy_connect_timeout 5s;
    proxy_read_timeout 5s;
    proxy_send_timeout 5s;
    proxy_next_upstream error timeout;
    proxy_next_upstream_time 10s;
    proxy_next_upstream_tries 2;
    proxy_pass http://backend;
    add_header upstream_addr $upstream_addr;
}
```













## 隔离术

### 线程隔离

根据业务等级将线程进行分配，尽可能的在大流量访问之间不被发生雪崩。

### 进程隔离

将一个大而全的系统划分成几个子系统，做到整个系统都是一部分，不会因为某个部分挂掉了，而影响全部的。

### 集群隔离

将单个服务成倍部署，从而均摊流量。

### 机房隔离



### 读写隔离

redis集群，主redis挂掉了，另外通过选举顶上主redis。

### 动静隔离

将静态资源抽离出来放在CDN上。

### 爬虫隔离

通过限流或者将爬虫流量定位到固定的集群中。

### 热点隔离

秒杀、抢购。

将这部分的业务抽离成一个独立的系统或者服务.从而不影响其他正常的系统业务。

### 资源隔离

### Hystrix实现隔离

如果一个接口访问非常缓慢,请求数量一旦非常大的情况下，会导致整个服务不可以用。线程永远处在等待中。

出现故障时，能够快速失败。或者返回托底的数据。

解决问题:

- 资源隔离，通过线程池和信号量
- 优雅降级:超时降级、资源不足时降级（线程和信号量），降级后配合降级接口返回托底数据。
- 熔断器实现:当失败率达到阀值自动触发降级。熔断器会触发快速失败并且会进行快速恢复。
- 请求缓存、请求合并实现。

## 限流详解

### 限流算法

### 令牌桶算法

一个存放固定容量的桶，按照固定速率往里面添加令牌。

桶中的令牌未满时可以继续存放

满了则丢弃..

### 漏桶算法

一个固定容量的桶，按照常量固定速率流出水滴。

如果桶是空的，则不需要流出水滴

如果流入的速度大于流出的速度,那么超过桶装的水将被丢弃，而漏桶的容量是不变的.

**两者的差别**

- 令牌桶是要查看桶中的令牌是否足够，为0时拒绝新的请求
- 漏桶则是按照固定的速率流出请求，只有当流入的速度大于流出的速度时,新的请求才会被拒绝。
- 令牌桶允许突发请求，只要有令牌就可以。
- 漏桶限制的是常量的速率，而不是常变的流出率。
- 令牌桶允许突发,而漏桶主要目的是平滑流入速率.

### 应用级限流

#### 限流总并发/连接/请求数

#### **tomcat**

```tex
acceptCount: 如果tomcat的线程都忙于响应,新来的连接就会进入队列排队,如果超出排队大小,则会拒绝连接
maxConnections : 瞬时最大连接数,超出的会排队等待。
maxThreads： tomcat能启动用来处理请求的最大线程数，如果请求处理量一直远远大于最大线程数，则会引起响应变慢甚至慢死。
```



#### 限流资源总数

连接池、线程池。

#### 限流某个接口的总并发/请求数

Java中的AtomicLong、Semaphore。

#### 限流某个接口的时间窗请求数

使用Guava的cache来存储计数器，过期时间为2秒。值为一个AtomicLong，每次获取的时候递增。两秒之后清空缓存。达到指定时间的最大并发数

#### 平滑限流某个接口的请求数

Guava框架框架提供的令牌算法可用于平滑突发限流(SmoothBursty)和平滑预热限流(SmoothWarmingUp)

### 节流

将多个重复的请求合并.

去掉连续事件.

例如100毫秒内如果触发了第二页和第三页操作，那么将第二页过滤掉。直接取最后一次事件的操作。

## 15. 队列术

### 15.1 应用场景

#### **异步处理**

比如用户注册成功后，需要发送注册邮件/用户积分/优惠券等等。

缓存过期时，先返回过期数据，然后异步更新缓存、异步写日志。

优点: 提升主流程的响应速度，而非主流程/非重要处理可以集中处理，将任务聚合批量处理。

#### **系统解耦**

用户支付完成订单后，需要通知生产配货系统、发票系统、库存系统、推荐系统、搜索系统等进行业务处理，而未来可能还需要更多的系统介入，因此可以通过消息队列来完成，将消息发送到队列中，需要的人自己订阅该消息的类型，进行相应业务的处理。

#### **数据同步**

Mysql的数据同步到Redis，或者将Mysql的数据同步到Mongodb，或者让机房之间的数据同步，或者主从数据同步。

#### **流量削峰**

将并发的流量通过队列有序的输入到应用中进行处理,保证了数据的有序性



### 15.2 **缓冲队列**

面对突发的流量，系统并不能够快速的处理，这时候需要将流量进行一个缓冲过渡到系统中，而不是全部压过来。

通过缓冲队列可以实现批量处理、异步处理和平滑流量

### 15.3 任务队列

一般是将一个复杂的任务进行拆解，然后统一丢到消息队列中，相应的消费者消费完成之后，将数据以邮件的方式通知客户。

优点: 

1. 异步任务 : 快速返回
2. 任务分解 : 利用多线程去处理每个任务.
3. 聚合处理 : 类似于jdk的forkJoinPool。

### 15.4 消息队列

常用的消息订阅模式:

1. 点对点

一个消息只有一个消费者

2. 发布订阅

一个消息对应的多个消费者

如果在事务中发MQ，会存在事务回滚，但是MQ发送成功了，则需要消息消费者进行幂等处理。

如果事务提交慢，但是MQ已经发出去了，则此时根据MQ信息再去获取**数据库数据可能不是最新**的。



















